{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Machine Learning for Huntington's Disease Prediction\n",
    "\n",
    "---\n",
    "\n",
    "**Objective:** Build and evaluate machine learning models to predict disease stage in Huntington's Disease patients using clinical, genetic, and molecular features.\n",
    "\n",
    "**Dataset:** Huntington's Disease Dataset (48,536 patients, 13 clinical features)\n",
    "\n",
    "**Target Variable:** Disease_Stage (5-class classification: Pre-symptomatic, Early Stage, Mid Stage, Late Stage, Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Accurate prediction of disease stage in Huntington's Disease enables:\n",
    "- **Early Intervention:** Identify patients who would benefit from early treatment\n",
    "- **Treatment Planning:** Tailor therapeutic strategies based on disease progression\n",
    "- **Clinical Trials:** Stratify patients for more effective trial enrollment\n",
    "- **Patient Counseling:** Provide evidence-based prognosis for personalized care\n",
    "\n",
    "---\n",
    "\n",
    "## Success Criteria\n",
    "\n",
    "- High classification accuracy (>85%)\n",
    "- Balanced precision and recall across all disease stages\n",
    "- Interpretable models that align with clinical knowledge\n",
    "- Robust generalization to unseen patient data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction & Setup\n",
    "\n",
    "### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import core libraries for data manipulation and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "#utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#setting random seed ensures reproducible results across runs\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "#configure display settings for better readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "#plotting style for visualizations\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ Core libraries imported successfully\")\n",
    "print(f\"✓ Random seed set to {RANDOM_STATE} for reproducibility\")\n",
    "print(\"\\nNote: Additional libraries will be imported in relevant sections as needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install required packages in jupyter kernel environment\n",
    "import sys\n",
    "!{sys.executable} -m pip install scikit-learn xgboost shap lime --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load cleaned data from assinment 2\n",
    "#data preprocessing (removing irrelevant columns, handling duplicates, etc) was completed in assignment 2\n",
    "df = pd.read_csv('data/Huntington_Disease_Cleaned.csv')\n",
    "\n",
    "print(f\"Data loaded: {df.shape[0]:,} patients, {df.shape[1]} features\")\n",
    "print(f\"Target variable: Disease_Stage (multi-class classification)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick overview\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check target variable distribution\n",
    "#check for class imbalance\n",
    "print(\"Disease Stage Distribution:\")\n",
    "print(df['Disease_Stage'].value_counts())\n",
    "print(f\"\\nClass balance:\")\n",
    "print(df['Disease_Stage'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize class distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "df['Disease_Stage'].value_counts().plot(kind='bar', color='steelblue')\n",
    "plt.title('Distribution of Disease Stages')\n",
    "plt.xlabel('Disease Stage')\n",
    "plt.ylabel('Number of Patients')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check current features\n",
    "print(\"Current features:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new features based on domain knowledge\n",
    "df_fe = df.copy()\n",
    "\n",
    "#genetic risk score: CAG repeats × gene expression\n",
    "df_fe['Genetic_Risk_Score'] = df_fe['HTT_CAG_Repeat_Length'] * df_fe['HTT_Gene_Expression_Level']\n",
    "\n",
    "#age-adjusted CAG: earlier onset = more aggressive\n",
    "df_fe['Age_Adjusted_CAG'] = df_fe['HTT_CAG_Repeat_Length'] / df_fe['Age']\n",
    "\n",
    "#brain health index: brain volume vs protein damage\n",
    "df_fe['Brain_Health_Index'] = (100 - df_fe['Brain_Volume_Loss']) / (df_fe['Protein_Aggregation_Level'] + 1)\n",
    "\n",
    "#motor-cognitive composite: combined symptom severity\n",
    "#using chorea score only since cognitive_decline is categorical\n",
    "df_fe['Motor_Cognitive_Composite'] = df_fe['Chorea_Score'] * df_fe['Brain_Volume_Loss']\n",
    "\n",
    "print(f\"Created 4 new features\")\n",
    "print(f\"Total features now: {df_fe.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check new features\n",
    "df_fe[['Genetic_Risk_Score', 'Age_Adjusted_CAG', 'Brain_Health_Index', 'Motor_Cognitive_Composite']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode categorical variables\n",
    "#one-hot encoding for nominal variables\n",
    "df_fe = pd.get_dummies(df_fe, columns=['Sex', 'Family_History', 'Motor_Symptoms', 'Cognitive_Decline'], drop_first=True)\n",
    "\n",
    "#label encoding for gene mutation type\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_fe['Gene_Mutation_Type_Encoded'] = le.fit_transform(df_fe['Gene_Mutation_Type'])\n",
    "df_fe = df_fe.drop('Gene_Mutation_Type', axis=1)\n",
    "\n",
    "print(f\"Encoded categorical variables\")\n",
    "print(f\"Total features after encoding: {df_fe.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check final feature list\n",
    "print(\"Final features after engineering:\")\n",
    "for i, col in enumerate(df_fe.columns, 1):\n",
    "    print(f\"{i}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate features and target\n",
    "X = df_fe.drop('Disease_Stage', axis=1)\n",
    "y = df_fe['Disease_Stage']\n",
    "\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Samples: {X.shape[0]}\")\n",
    "print(f\"Target classes: {y.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method 1: ANOVA f-test\n",
    "#tests relationship between each feature and target\n",
    "#works well for numerical features in classification\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=10)\n",
    "selector.fit(X, y)\n",
    "\n",
    "anova_features = X.columns[selector.get_support()].tolist()\n",
    "print(\"Top 10 features by ANOVA:\")\n",
    "for i, feat in enumerate(anova_features, 1):\n",
    "    print(f\"{i}. {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method 2: mutual information\n",
    "#captures non-linear relationships between features and target\n",
    "#complements ANOVA which only finds linear relationships\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "mi_scores = mutual_info_classif(X, y, random_state=RANDOM_STATE)\n",
    "mi_features = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 10 features by Mutual Information:\")\n",
    "for i, (feat, score) in enumerate(mi_features.head(10).items(), 1):\n",
    "    print(f\"{i}. {feat}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method 3: random forest importance\n",
    "#embedded method that considers feature interactions\n",
    "#importance based on how much each feature improves tree splits\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "rf.fit(X, y)\n",
    "\n",
    "rf_importance = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 10 features by Random Forest:\")\n",
    "for i, (feat, score) in enumerate(rf_importance.head(10).items(), 1):\n",
    "    print(f\"{i}. {feat}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find consensus features across methods\n",
    "#features appearing in top 10 of at least 2 methods are most reliable\n",
    "anova_top10 = set(anova_features)\n",
    "mi_top10 = set(mi_features.head(10).index)\n",
    "rf_top10 = set(rf_importance.head(10).index)\n",
    "\n",
    "#count how many methods selected each feature\n",
    "all_features = anova_top10 | mi_top10 | rf_top10\n",
    "feature_counts = {}\n",
    "for feat in all_features:\n",
    "    count = 0\n",
    "    if feat in anova_top10: count += 1\n",
    "    if feat in mi_top10: count += 1\n",
    "    if feat in rf_top10: count += 1\n",
    "    feature_counts[feat] = count\n",
    "\n",
    "#select features with consensus (appear in 2+ methods)\n",
    "selected_features = [feat for feat, count in feature_counts.items() if count >= 2]\n",
    "\n",
    "print(f\"\\nConsensus features (≥2 methods): {len(selected_features)}\")\n",
    "for feat in sorted(selected_features):\n",
    "    methods = []\n",
    "    if feat in anova_top10: methods.append('ANOVA')\n",
    "    if feat in mi_top10: methods.append('MI')\n",
    "    if feat in rf_top10: methods.append('RF')\n",
    "    print(f\"  {feat} [{', '.join(methods)}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create final dataset with selected features\n",
    "X_selected = X[selected_features]\n",
    "\n",
    "print(f\"\\nOriginal features: {X.shape[1]}\")\n",
    "print(f\"Selected features: {X_selected.shape[1]}\")\n",
    "print(f\"Reduction: {((X.shape[1] - X_selected.shape[1]) / X.shape[1] * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train and test sets\n",
    "#stratify maintains class balance in both sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_selected, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale features for models that need it\n",
    "#svm and knn are sensitive to feature scales\n",
    "#tree-based models (random forest) don't need scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features scaled (mean=0, std=1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 1: logistic regression (baseline)\n",
    "#simple linear model, interpretable\n",
    "#provides probability estimates for clinical decisions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lr.predict(X_test_scaled)\n",
    "\n",
    "print(\"Logistic Regression:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_lr, average='weighted'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 2: random forest\n",
    "#ensemble method, handles non-linear relationships\n",
    "#robust to outliers, provides feature importance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "print(\"Random Forest:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_rf, average='weighted'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: xgboost has import error - check this one later\n",
    "# maybe find another alternative\n",
    "print(\"XGBoost: Skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 3: support vector machine\n",
    "#effective in high-dimensional space\n",
    "#rbf kernel captures non-linear patterns\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='rbf', random_state=RANDOM_STATE)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"Support Vector Machine:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_svm):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_svm, average='weighted'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 4: k-nearest neighbors\n",
    "#non-parametric, simple and interpretable\n",
    "#classifies based on similarity to training samples\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "y_pred_knn = knn_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"K-Nearest Neighbors:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_knn):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_knn, average='weighted'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare all models\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'SVM', 'KNN'],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_test, y_pred_lr),\n",
    "        accuracy_score(y_test, y_pred_rf),\n",
    "        accuracy_score(y_test, y_pred_svm),\n",
    "        accuracy_score(y_test, y_pred_knn)\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        f1_score(y_test, y_pred_lr, average='weighted'),\n",
    "        f1_score(y_test, y_pred_rf, average='weighted'),\n",
    "        f1_score(y_test, y_pred_svm, average='weighted'),\n",
    "        f1_score(y_test, y_pred_knn, average='weighted')\n",
    "    ]\n",
    "})\n",
    "\n",
    "results = results.sort_values('Accuracy', ascending=False)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#detailed metrics for each model\n",
    "#precision: how many predicted positives are actually positive (important for clinical decisions)\n",
    "#recall: how many actual positives we caught (important for early detection)\n",
    "#f1-score: harmonic mean balancing precision and recall\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"=== LOGISTIC REGRESSION ===\")\n",
    "print(classification_report(y_test, y_pred_lr))\n",
    "\n",
    "print(\"\\n=== RANDOM FOREST ===\")\n",
    "print(classification_report(y_test, y_pred_rf))\n",
    "\n",
    "print(\"\\n=== SUPPORT VECTOR MACHINE ===\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "\n",
    "print(\"\\n=== K-NEAREST NEIGHBORS ===\")\n",
    "print(classification_report(y_test, y_pred_knn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrices show which stages get confused\n",
    "#diagonal = correct predictions, off-diagonal = errors\n",
    "#helps identify if model confuses adjacent stages (early vs mid)\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "models = [\n",
    "    ('Logistic Regression', y_pred_lr),\n",
    "    ('Random Forest', y_pred_rf),\n",
    "    ('SVM', y_pred_svm),\n",
    "    ('KNN', y_pred_knn)\n",
    "]\n",
    "\n",
    "for idx, (name, y_pred) in enumerate(models):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "    ax.set_title(f'{name}')\n",
    "    ax.set_ylabel('Actual Stage')\n",
    "    ax.set_xlabel('Predicted Stage')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#roc curves and auc scores\n",
    "#shows trade-off between true positive rate and false positive rate\n",
    "#auc closer to 1.0 = better discrimination between classes\n",
    "#use one-vs-rest for multi-class\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "#binarize labels for multi-class roc\n",
    "y_test_bin = label_binarize(y_test, classes=sorted(y_test.unique()))\n",
    "n_classes = y_test_bin.shape[1]\n",
    "\n",
    "#get probability predictions for random forest (best model)\n",
    "y_score_rf = rf_model.predict_proba(X_test)\n",
    "\n",
    "#compute roc curve and auc for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_score_rf[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "#plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "stages = sorted(y_test.unique())\n",
    "\n",
    "for i, color, stage in zip(range(n_classes), colors, stages):\n",
    "    plt.plot(fpr[i], tpr[i], color=color, lw=2,\n",
    "             label=f'{stage} (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Chance')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves - Random Forest (One-vs-Rest)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model performance comparison across metrics\n",
    "#helps identify best model for clinical use\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'SVM', 'KNN'],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_test, y_pred_lr),\n",
    "        accuracy_score(y_test, y_pred_rf),\n",
    "        accuracy_score(y_test, y_pred_svm),\n",
    "        accuracy_score(y_test, y_pred_knn)\n",
    "    ],\n",
    "    'Precision': [\n",
    "        precision_score(y_test, y_pred_lr, average='weighted'),\n",
    "        precision_score(y_test, y_pred_rf, average='weighted'),\n",
    "        precision_score(y_test, y_pred_svm, average='weighted'),\n",
    "        precision_score(y_test, y_pred_knn, average='weighted')\n",
    "    ],\n",
    "    'Recall': [\n",
    "        recall_score(y_test, y_pred_lr, average='weighted'),\n",
    "        recall_score(y_test, y_pred_rf, average='weighted'),\n",
    "        recall_score(y_test, y_pred_svm, average='weighted'),\n",
    "        recall_score(y_test, y_pred_knn, average='weighted')\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        f1_score(y_test, y_pred_lr, average='weighted'),\n",
    "        f1_score(y_test, y_pred_rf, average='weighted'),\n",
    "        f1_score(y_test, y_pred_svm, average='weighted'),\n",
    "        f1_score(y_test, y_pred_knn, average='weighted')\n",
    "    ]\n",
    "})\n",
    "\n",
    "comparison = comparison.sort_values('Accuracy', ascending=False)\n",
    "print(\"Comprehensive Model Comparison:\")\n",
    "print(comparison.to_string(index=False))\n",
    "\n",
    "#visualize\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x = np.arange(len(comparison))\n",
    "width = 0.2\n",
    "\n",
    "ax.bar(x - 1.5*width, comparison['Accuracy'], width, label='Accuracy', color='skyblue')\n",
    "ax.bar(x - 0.5*width, comparison['Precision'], width, label='Precision', color='lightcoral')\n",
    "ax.bar(x + 0.5*width, comparison['Recall'], width, label='Recall', color='lightgreen')\n",
    "ax.bar(x + 1.5*width, comparison['F1-Score'], width, label='F1-Score', color='gold')\n",
    "\n",
    "ax.set_xlabel('Model')\n",
    "ax.set_ylabel('Score')\n",
    "ax.set_title('Model Performance Comparison Across Metrics')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(comparison['Model'], rotation=45, ha='right')\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Overfitting/Underfitting Prevention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cross-validation ensures model performs well on unseen data\n",
    "#k-fold splits data into k parts, trains on k-1, tests on 1, repeats k times\n",
    "#gives more reliable estimate than single train/test split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "models_cv = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=RANDOM_STATE),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    'SVM': SVC(kernel='rbf', random_state=RANDOM_STATE),\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "print(\"5-Fold Cross-Validation Results:\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "cv_results = {}\n",
    "for name, model in models_cv.items():\n",
    "    #use scaled data for LR, SVM, KNN; original for RF\n",
    "    if name in ['Logistic Regression', 'SVM', 'KNN']:\n",
    "        X_cv = scaler.fit_transform(X_selected)\n",
    "    else:\n",
    "        X_cv = X_selected\n",
    "    \n",
    "    scores = cross_val_score(model, X_cv, y, cv=5, scoring='accuracy')\n",
    "    cv_results[name] = scores\n",
    "    \n",
    "    print(f\"{name}:\")\n",
    "    print(f\"  Mean Accuracy: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
    "    print(f\"  All Folds: {[f'{s:.4f}' for s in scores]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learning curves show if model is overfitting or underfitting\n",
    "#overfitting: large gap between train and validation scores (model memorizes)\n",
    "#underfitting: both scores are low (model too simple)\n",
    "#good fit: both scores high and close together\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "models_lc = [\n",
    "    ('Logistic Regression', lr, X_train_scaled, X_test_scaled),\n",
    "    ('Random Forest', rf_model, X_train, X_test),\n",
    "    ('SVM', svm_model, X_train_scaled, X_test_scaled),\n",
    "    ('KNN', knn_model, X_train_scaled, X_test_scaled)\n",
    "]\n",
    "\n",
    "for idx, (name, model, X_tr, X_te) in enumerate(models_lc):\n",
    "    ax = axes[idx // 2, idx % 2]\n",
    "    \n",
    "    #calculate learning curve\n",
    "    train_sizes, train_scores, val_scores = learning_curve(\n",
    "        model, X_tr, y_train, cv=5, n_jobs=-1,\n",
    "        train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "        scoring='accuracy'\n",
    "    )\n",
    "    \n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    train_std = np.std(train_scores, axis=1)\n",
    "    val_mean = np.mean(val_scores, axis=1)\n",
    "    val_std = np.std(val_scores, axis=1)\n",
    "    \n",
    "    ax.plot(train_sizes, train_mean, label='Training Score', color='blue', marker='o')\n",
    "    ax.fill_between(train_sizes, train_mean - train_std, train_mean + train_std, alpha=0.1, color='blue')\n",
    "    \n",
    "    ax.plot(train_sizes, val_mean, label='Validation Score', color='red', marker='s')\n",
    "    ax.fill_between(train_sizes, val_mean - val_std, val_mean + val_std, alpha=0.1, color='red')\n",
    "    \n",
    "    ax.set_xlabel('Training Set Size')\n",
    "    ax.set_ylabel('Accuracy')\n",
    "    ax.set_title(f'Learning Curve - {name}')\n",
    "    ax.legend(loc='lower right')\n",
    "    ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regularization for logistic regression prevents overfitting\n",
    "#l2 penalty shrinks coefficients, reducing model complexity\n",
    "#compare different C values (smaller C = stronger regularization)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "C_values = [0.001, 0.01, 0.1, 1, 10, 100]\n",
    "train_scores_reg = []\n",
    "test_scores_reg = []\n",
    "\n",
    "for C in C_values:\n",
    "    lr_reg = LogisticRegression(C=C, max_iter=1000, random_state=RANDOM_STATE)\n",
    "    lr_reg.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    train_score = lr_reg.score(X_train_scaled, y_train)\n",
    "    test_score = lr_reg.score(X_test_scaled, y_test)\n",
    "    \n",
    "    train_scores_reg.append(train_score)\n",
    "    test_scores_reg.append(test_score)\n",
    "\n",
    "#plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogx(C_values, train_scores_reg, label='Training Score', marker='o', color='blue')\n",
    "plt.semilogx(C_values, test_scores_reg, label='Test Score', marker='s', color='red')\n",
    "plt.xlabel('C (Inverse Regularization Strength)')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Regularization Effect on Logistic Regression')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Regularization Analysis:\")\n",
    "for C, train, test in zip(C_values, train_scores_reg, test_scores_reg):\n",
    "    print(f\"C={C:7.3f}: Train={train:.4f}, Test={test:.4f}, Gap={abs(train-test):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter tuning for random forest prevents overfitting\n",
    "#grid search finds best combination of parameters\n",
    "#max_depth limits tree depth, min_samples_split controls splitting\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 30, None],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "\n",
    "rf_grid = GridSearchCV(\n",
    "    RandomForestClassifier(random_state=RANDOM_STATE, n_jobs=-1),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "rf_grid.fit(X_train, y_train)\n",
    "\n",
    "print(\"\\nBest Parameters:\")\n",
    "for param, value in rf_grid.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "print(f\"\\nBest Cross-Validation Score: {rf_grid.best_score_:.4f}\")\n",
    "print(f\"Test Set Score: {rf_grid.score(X_test, y_test):.4f}\")\n",
    "\n",
    "#compare with default\n",
    "print(f\"\\nImprovement over default RF:\")\n",
    "print(f\"  Default Test Score: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"  Tuned Test Score: {rf_grid.score(X_test, y_test):.4f}\")\n",
    "print(f\"  Difference: {rf_grid.score(X_test, y_test) - accuracy_score(y_test, y_pred_rf):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save tuned model for later use\n",
    "best_rf_model = rf_grid.best_estimator_\n",
    "y_pred_rf_tuned = best_rf_model.predict(X_test)\n",
    "\n",
    "print(\"Tuned Random Forest Performance:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf_tuned):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_rf_tuned, average='weighted'):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
