{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3: Machine Learning for Huntington's Disease Prediction\n",
    "\n",
    "---\n",
    "\n",
    "**Objective:** Build and evaluate machine learning models to predict disease stage in Huntington's Disease patients using clinical, genetic, and molecular features.\n",
    "\n",
    "**Dataset:** Huntington's Disease Dataset (48,536 patients, 13 clinical features)\n",
    "\n",
    "**Target Variable:** Disease_Stage (5-class classification: Pre-symptomatic, Early Stage, Mid Stage, Late Stage, Advanced)\n",
    "\n",
    "---\n",
    "\n",
    "## Why This Matters\n",
    "\n",
    "Accurate prediction of disease stage in Huntington's Disease enables:\n",
    "- **Early Intervention:** Identify patients who would benefit from early treatment\n",
    "- **Treatment Planning:** Tailor therapeutic strategies based on disease progression\n",
    "- **Clinical Trials:** Stratify patients for more effective trial enrollment\n",
    "- **Patient Counseling:** Provide evidence-based prognosis for personalized care\n",
    "\n",
    "---\n",
    "\n",
    "## Success Criteria\n",
    "\n",
    "- High classification accuracy (>85%)\n",
    "- Balanced precision and recall across all disease stages\n",
    "- Interpretable models that align with clinical knowledge\n",
    "- Robust generalization to unseen patient data\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction & Setup\n",
    "\n",
    "### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import core libraries for data manipulation and visualization\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "#utilities\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#setting random seed ensures reproducible results across runs\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "#configure display settings for better readability\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.4f}'.format)\n",
    "\n",
    "#plotting style for visualizations\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✓ Core libraries imported successfully\")\n",
    "print(f\"✓ Random seed set to {RANDOM_STATE} for reproducibility\")\n",
    "print(\"\\nNote: Additional libraries will be imported in relevant sections as needed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install required packages in jupyter kernel environment\n",
    "import sys\n",
    "!{sys.executable} -m pip install scikit-learn xgboost shap lime --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load cleaned data from assinment 2\n",
    "#data preprocessing (removing irrelevant columns, handling duplicates, etc) was completed in assignment 2\n",
    "df = pd.read_csv('data/Huntington_Disease_Cleaned.csv')\n",
    "\n",
    "print(f\"Data loaded: {df.shape[0]:,} patients, {df.shape[1]} features\")\n",
    "print(f\"Target variable: Disease_Stage (multi-class classification)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#quick overview\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check target variable distribution\n",
    "#check for class imbalance\n",
    "print(\"Disease Stage Distribution:\")\n",
    "print(df['Disease_Stage'].value_counts())\n",
    "print(f\"\\nClass balance:\")\n",
    "print(df['Disease_Stage'].value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#visualize class distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "df['Disease_Stage'].value_counts().plot(kind='bar', color='steelblue')\n",
    "plt.title('Distribution of Disease Stages')\n",
    "plt.xlabel('Disease Stage')\n",
    "plt.ylabel('Number of Patients')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check current features\n",
    "print(\"Current features:\")\n",
    "for i, col in enumerate(df.columns, 1):\n",
    "    print(f\"{i}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create new features based on domain knowledge\n",
    "df_fe = df.copy()\n",
    "\n",
    "#genetic risk score: CAG repeats × gene expression\n",
    "df_fe['Genetic_Risk_Score'] = df_fe['HTT_CAG_Repeat_Length'] * df_fe['HTT_Gene_Expression_Level']\n",
    "\n",
    "#age-adjusted CAG: earlier onset = more aggressive\n",
    "df_fe['Age_Adjusted_CAG'] = df_fe['HTT_CAG_Repeat_Length'] / df_fe['Age']\n",
    "\n",
    "#brain health index: brain volume vs protein damage\n",
    "df_fe['Brain_Health_Index'] = (100 - df_fe['Brain_Volume_Loss']) / (df_fe['Protein_Aggregation_Level'] + 1)\n",
    "\n",
    "#motor-cognitive composite: combined symptom severity\n",
    "#using chorea score only since cognitive_decline is categorical\n",
    "df_fe['Motor_Cognitive_Composite'] = df_fe['Chorea_Score'] * df_fe['Brain_Volume_Loss']\n",
    "\n",
    "print(f\"Created 4 new features\")\n",
    "print(f\"Total features now: {df_fe.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check new features\n",
    "df_fe[['Genetic_Risk_Score', 'Age_Adjusted_CAG', 'Brain_Health_Index', 'Motor_Cognitive_Composite']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode categorical variables\n",
    "#one-hot encoding for nominal variables\n",
    "df_fe = pd.get_dummies(df_fe, columns=['Sex', 'Family_History', 'Motor_Symptoms', 'Cognitive_Decline'], drop_first=True)\n",
    "\n",
    "#label encoding for gene mutation type\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_fe['Gene_Mutation_Type_Encoded'] = le.fit_transform(df_fe['Gene_Mutation_Type'])\n",
    "df_fe = df_fe.drop('Gene_Mutation_Type', axis=1)\n",
    "\n",
    "print(f\"Encoded categorical variables\")\n",
    "print(f\"Total features after encoding: {df_fe.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check final feature list\n",
    "print(\"Final features after engineering:\")\n",
    "for i, col in enumerate(df_fe.columns, 1):\n",
    "    print(f\"{i}. {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separate features and target\n",
    "X = df_fe.drop('Disease_Stage', axis=1)\n",
    "y = df_fe['Disease_Stage']\n",
    "\n",
    "print(f\"Features: {X.shape[1]}\")\n",
    "print(f\"Samples: {X.shape[0]}\")\n",
    "print(f\"Target classes: {y.nunique()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method 1: ANOVA f-test\n",
    "#tests relationship between each feature and target\n",
    "#works well for numerical features in classification\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "selector = SelectKBest(score_func=f_classif, k=10)\n",
    "selector.fit(X, y)\n",
    "\n",
    "anova_features = X.columns[selector.get_support()].tolist()\n",
    "print(\"Top 10 features by ANOVA:\")\n",
    "for i, feat in enumerate(anova_features, 1):\n",
    "    print(f\"{i}. {feat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method 2: mutual information\n",
    "#captures non-linear relationships between features and target\n",
    "#complements ANOVA which only finds linear relationships\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "mi_scores = mutual_info_classif(X, y, random_state=RANDOM_STATE)\n",
    "mi_features = pd.Series(mi_scores, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 10 features by Mutual Information:\")\n",
    "for i, (feat, score) in enumerate(mi_features.head(10).items(), 1):\n",
    "    print(f\"{i}. {feat}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method 3: random forest importance\n",
    "#embedded method that considers feature interactions\n",
    "#importance based on how much each feature improves tree splits\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "rf.fit(X, y)\n",
    "\n",
    "rf_importance = pd.Series(rf.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "\n",
    "print(\"Top 10 features by Random Forest:\")\n",
    "for i, (feat, score) in enumerate(rf_importance.head(10).items(), 1):\n",
    "    print(f\"{i}. {feat}: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find consensus features across methods\n",
    "#features appearing in top 10 of at least 2 methods are most reliable\n",
    "anova_top10 = set(anova_features)\n",
    "mi_top10 = set(mi_features.head(10).index)\n",
    "rf_top10 = set(rf_importance.head(10).index)\n",
    "\n",
    "#count how many methods selected each feature\n",
    "all_features = anova_top10 | mi_top10 | rf_top10\n",
    "feature_counts = {}\n",
    "for feat in all_features:\n",
    "    count = 0\n",
    "    if feat in anova_top10: count += 1\n",
    "    if feat in mi_top10: count += 1\n",
    "    if feat in rf_top10: count += 1\n",
    "    feature_counts[feat] = count\n",
    "\n",
    "#select features with consensus (appear in 2+ methods)\n",
    "selected_features = [feat for feat, count in feature_counts.items() if count >= 2]\n",
    "\n",
    "print(f\"\\nConsensus features (≥2 methods): {len(selected_features)}\")\n",
    "for feat in sorted(selected_features):\n",
    "    methods = []\n",
    "    if feat in anova_top10: methods.append('ANOVA')\n",
    "    if feat in mi_top10: methods.append('MI')\n",
    "    if feat in rf_top10: methods.append('RF')\n",
    "    print(f\"  {feat} [{', '.join(methods)}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create final dataset with selected features\n",
    "X_selected = X[selected_features]\n",
    "\n",
    "print(f\"\\nOriginal features: {X.shape[1]}\")\n",
    "print(f\"Selected features: {X_selected.shape[1]}\")\n",
    "print(f\"Reduction: {((X.shape[1] - X_selected.shape[1]) / X.shape[1] * 100):.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Machine Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into train and test sets\n",
    "#stratify maintains class balance in both sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_selected, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "print(f\"Features: {X_train.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale features for models that need it\n",
    "#svm and knn are sensitive to feature scales\n",
    "#tree-based models (random forest) don't need scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(\"Features scaled (mean=0, std=1)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 1: logistic regression (baseline)\n",
    "#simple linear model, interpretable\n",
    "#provides probability estimates for clinical decisions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "lr = LogisticRegression(max_iter=1000, random_state=RANDOM_STATE)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lr.predict(X_test_scaled)\n",
    "\n",
    "print(\"Logistic Regression:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_lr):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_lr, average='weighted'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 2: random forest\n",
    "#ensemble method, handles non-linear relationships\n",
    "#robust to outliers, provides feature importance\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE, n_jobs=-1)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "print(\"Random Forest:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_rf, average='weighted'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: xgboost has import error - check this one later\n",
    "# maybe find another alternative\n",
    "print(\"XGBoost: Skipped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 3: support vector machine\n",
    "#effective in high-dimensional space\n",
    "#rbf kernel captures non-linear patterns\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_model = SVC(kernel='rbf', random_state=RANDOM_STATE)\n",
    "svm_model.fit(X_train_scaled, y_train)\n",
    "y_pred_svm = svm_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"Support Vector Machine:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_svm):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_svm, average='weighted'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model 4: k-nearest neighbors\n",
    "#non-parametric, simple and interpretable\n",
    "#classifies based on similarity to training samples\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "y_pred_knn = knn_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"K-Nearest Neighbors:\")\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred_knn):.4f}\")\n",
    "print(f\"F1-Score: {f1_score(y_test, y_pred_knn, average='weighted'):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compare all models\n",
    "results = pd.DataFrame({\n",
    "    'Model': ['Logistic Regression', 'Random Forest', 'SVM', 'KNN'],\n",
    "    'Accuracy': [\n",
    "        accuracy_score(y_test, y_pred_lr),\n",
    "        accuracy_score(y_test, y_pred_rf),\n",
    "        accuracy_score(y_test, y_pred_svm),\n",
    "        accuracy_score(y_test, y_pred_knn)\n",
    "    ],\n",
    "    'F1-Score': [\n",
    "        f1_score(y_test, y_pred_lr, average='weighted'),\n",
    "        f1_score(y_test, y_pred_rf, average='weighted'),\n",
    "        f1_score(y_test, y_pred_svm, average='weighted'),\n",
    "        f1_score(y_test, y_pred_knn, average='weighted')\n",
    "    ]\n",
    "})\n",
    "\n",
    "results = results.sort_values('Accuracy', ascending=False)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(results.to_string(index=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
