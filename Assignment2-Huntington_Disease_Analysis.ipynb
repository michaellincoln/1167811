{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Huntington's Disease Dataset Analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and analysis libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options for better DataFrame output\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Set plotting style for better visualizations\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv('data/Huntington_Disease_Dataset.csv')\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of patients: {df.shape[0]}\")\n",
    "print(f\"Number of features: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display first few rows to see the data structure\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#detailed information about the dataset\n",
    "#to check data types, missing values, and memory usage\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\" * 50)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#statistical summary of numerical columns\n",
    "#to provide insights into the distribution of key clinical variables\n",
    "print(\"Statistical Summary of Numerical Features:\")\n",
    "print(\"=\" * 50)\n",
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify column types and their relevance for analysis\n",
    "\n",
    "print(\"Column Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "for i, col in enumerate(df.columns):\n",
    "    dtype = df[col].dtype\n",
    "    unique_vals = df[col].nunique()\n",
    "    missing_vals = df[col].isnull().sum()\n",
    "    print(f\"{i+1:2d}. {col:<30} | {str(dtype):<10} | Unique: {unique_vals:4d} | Missing: {missing_vals:3d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "### 3.1 Data Cleaning and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test to remove irrelevant columns for analysis\n",
    "#patient_ID: unique identifier, not predictive\n",
    "#random sequences: generated for privacy, not real biological data\n",
    "#gene info columns: redundant descriptive information\n",
    "#\n",
    "\n",
    "columns_to_drop = [\n",
    "    'Patient_ID',  # Unique identifier - not predictive\n",
    "    'Random_Protein_Sequence',  # Random sequence for privacy\n",
    "    'Random_Gene_Sequence',  # Random sequence for privacy  \n",
    "    'Gene/Factor',  # Redundant with other genetic features\n",
    "    'Chromosome_Location',  # Static genetic information\n",
    "    'Function',  # Descriptive, not quantitative\n",
    "    'Effect',  # Descriptive, not quantitative\n",
    "    'Category'  # Descriptive, not quantitative\n",
    "]\n",
    "\n",
    "# Create cleaned dataset focusing on clinically relevant features\n",
    "df_clean = df.drop(columns=columns_to_drop)\n",
    "\n",
    "print(f\"Original dataset: {df.shape}\")\n",
    "print(f\"Cleaned dataset: {df_clean.shape}\")\n",
    "print(f\"Removed {len(columns_to_drop)} irrelevant columns\")\n",
    "\n",
    "print(\"\\nRemaining features:\")\n",
    "for col in df_clean.columns:\n",
    "    print(f\"- {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Handle Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate records\n",
    "# In medical data, duplicates could indicate data entry errors\n",
    "\n",
    "print(\"Duplicate Analysis:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Check for complete duplicates\n",
    "duplicate_rows = df_clean.duplicated().sum()\n",
    "print(f\"Complete duplicate rows: {duplicate_rows}\")\n",
    "\n",
    "# Check for duplicates based on key clinical features\n",
    "key_features = ['Age', 'Sex', 'HTT_CAG_Repeat_Length', 'Disease_Stage']\n",
    "duplicate_clinical = df_clean.duplicated(subset=key_features).sum()\n",
    "print(f\"Duplicate clinical profiles: {duplicate_clinical}\")\n",
    "\n",
    "if duplicate_rows > 0:\n",
    "    print(f\"\\nRemoving {duplicate_rows} duplicate rows...\")\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    print(f\"Dataset shape after removing duplicates: {df_clean.shape}\")\n",
    "else:\n",
    "    print(\"No duplicate rows found - data quality is good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Missing Data Analysis and Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing data patterns\n",
    "plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Missing data heatmap\n",
    "plt.subplot(2, 2, 1)\n",
    "# Create shortened labels for better readability\n",
    "short_labels = [\n",
    "    'Age', 'Sex', 'Family_Hist', 'HTT_CAG', 'Motor_Symp', \n",
    "    'Cognitive', 'Chorea', 'Brain_Vol', 'Functional', \n",
    "    'Gene_Mut', 'HTT_Expr', 'Protein_Agg', 'Disease_Stage'\n",
    "]\n",
    "\n",
    "# Create heatmap with improved formatting\n",
    "ax1 = sns.heatmap(df_clean.isnull(), \n",
    "                  cbar=True, \n",
    "                  xticklabels=short_labels,\n",
    "                  yticklabels=False,\n",
    "                  cmap='viridis', \n",
    "                  cbar_kws={'label': 'Missing Data'})\n",
    "plt.title('Missing Data Heatmap', fontsize=14, pad=15)\n",
    "plt.xlabel('Clinical Features', fontsize=12)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=10)\n",
    "\n",
    "# Missing data bar plot\n",
    "plt.subplot(2, 2, 2)\n",
    "missing_counts = df_clean.isnull().sum().sort_values(ascending=True)\n",
    "missing_counts = missing_counts[missing_counts > 0] \n",
    "if len(missing_counts) > 0:\n",
    "    missing_counts.plot(kind='barh', color='coral')\n",
    "    plt.title('Missing Data Count by Feature')\n",
    "    plt.xlabel('Number of Missing Values')\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'No Missing Data Found!', ha='center', va='center', fontsize=14)\n",
    "    plt.title('Missing Data Count by Feature')\n",
    "\n",
    "# Missing data percentage\n",
    "plt.subplot(2, 2, 3)\n",
    "missing_percentages = ((df_clean.isnull().sum() / len(df_clean)) * 100).sort_values(ascending=True)\n",
    "missing_percentages = missing_percentages[missing_percentages > 0]\n",
    "if len(missing_percentages) > 0:\n",
    "    missing_percentages.plot(kind='barh', color='lightblue')\n",
    "    plt.title('Missing Data Percentage by Feature')\n",
    "    plt.xlabel('Percentage of Missing Values (%)')\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'No Missing Data Found!', ha='center', va='center', fontsize=14)\n",
    "    plt.title('Missing Data Percentage by Feature')\n",
    "\n",
    "# Data completeness overview\n",
    "plt.subplot(2, 2, 4)\n",
    "total_cells = len(df_clean) * len(df_clean.columns)\n",
    "missing_cells = df_clean.isnull().sum().sum()\n",
    "complete_cells = total_cells - missing_cells\n",
    "\n",
    "labels = ['Complete', 'Missing']\n",
    "sizes = [complete_cells, missing_cells]\n",
    "colors = ['lightgreen', 'lightcoral']\n",
    "\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Overall Data Completeness')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nData Completeness Summary:\")\n",
    "print(f\"Total data points: {total_cells:,}\")\n",
    "print(f\"Complete data points: {complete_cells:,} ({(complete_cells/total_cells)*100:.1f}%)\")\n",
    "print(f\"Missing data points: {missing_cells:,} ({(missing_cells/total_cells)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive missing data analysis\n",
    "print(\"Missing Data Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Calculate missing data statistics\n",
    "missing_stats = []\n",
    "for col in df_clean.columns:\n",
    "    missing_count = df_clean[col].isnull().sum()\n",
    "    missing_percent = (missing_count / len(df_clean)) * 100\n",
    "    missing_stats.append({\n",
    "        'Column': col,\n",
    "        'Missing_Count': missing_count,\n",
    "        'Missing_Percent': round(missing_percent, 2),\n",
    "        'Data_Type': str(df_clean[col].dtype)\n",
    "    })\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "missing_df = pd.DataFrame(missing_stats)\n",
    "missing_df = missing_df.sort_values('Missing_Percent', ascending=False)\n",
    "\n",
    "print(\"Missing Data Summary:\")\n",
    "print(missing_df.to_string(index=False))\n",
    "\n",
    "# Identify columns with significant missing data (>5% missing)\n",
    "high_missing = missing_df[missing_df['Missing_Percent'] > 5]\n",
    "print(f\"\\nColumns with >5% missing data:\")\n",
    "if len(high_missing) > 0:\n",
    "    for _, row in high_missing.iterrows():\n",
    "        print(f\"- {row['Column']}: {row['Missing_Count']} missing ({row['Missing_Percent']}%)\")\n",
    "else:\n",
    "    print(\"- None (excellent data quality!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Outlier Detection and Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection\n",
    "# In medical data, outliers could represent: rare but valid extreme cases, data entry errors, measurement equipment issues\n",
    "\n",
    "def detect_outliers_iqr(data, column):\n",
    "    \"\"\"Detect outliers using Interquartile Range (IQR) method\"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "def detect_outliers_zscore(data, column, threshold=3):\n",
    "    \"\"\"Detect outliers using Z-score method\"\"\"\n",
    "    z_scores = np.abs(stats.zscore(data[column].dropna()))\n",
    "    outliers = data[np.abs(stats.zscore(data[column].dropna())) > threshold]\n",
    "    return outliers, z_scores\n",
    "\n",
    "# Identify numerical columns for outlier analysis\n",
    "numerical_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(\"Numerical columns for outlier detection:\")\n",
    "for i, col in enumerate(numerical_cols, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "print(f\"\\nAnalyzing {len(numerical_cols)} numerical features for outliers...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive outlier analysis\n",
    "outlier_summary = []\n",
    "\n",
    "print(\"Outlier Detection Summary:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Feature':<25} {'Total':<8} {'IQR_Out':<8} {'Z_Out':<8} {'%_IQR':<8} {'%_Z':<8}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for col in numerical_cols:\n",
    "    # Only analyze columns with data\n",
    "    if df_clean[col].notna().sum() > 0:  \n",
    "        # IQR method\n",
    "        iqr_outliers, lower_iqr, upper_iqr = detect_outliers_iqr(df_clean, col)\n",
    "        \n",
    "        # Z-score method  \n",
    "        zscore_outliers, z_scores = detect_outliers_zscore(df_clean, col)\n",
    "        \n",
    "        # Calculate percentages\n",
    "        total_valid = df_clean[col].notna().sum()\n",
    "        iqr_pct = (len(iqr_outliers) / total_valid) * 100\n",
    "        z_pct = (len(zscore_outliers) / total_valid) * 100\n",
    "        \n",
    "        # Store summary\n",
    "        outlier_summary.append({\n",
    "            'Feature': col,\n",
    "            'Total_Records': total_valid,\n",
    "            'IQR_Outliers': len(iqr_outliers),\n",
    "            'Z_Outliers': len(zscore_outliers),\n",
    "            'IQR_Percentage': round(iqr_pct, 2),\n",
    "            'Z_Percentage': round(z_pct, 2),\n",
    "            'Lower_Bound_IQR': round(lower_iqr, 2),\n",
    "            'Upper_Bound_IQR': round(upper_iqr, 2)\n",
    "        })\n",
    "        \n",
    "        print(f\"{col:<25} {total_valid:<8} {len(iqr_outliers):<8} {len(zscore_outliers):<8} {iqr_pct:<8.1f} {z_pct:<8.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to DataFrame for better analysis\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Define clinical significance thresholds for medical data\n",
    "print(\"OUTLIER INTERPRETATION GUIDELINES:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"• <2%: Excellent data quality - minimal outliers expected\")\n",
    "print(\"• 2-5%: Good data quality - acceptable outlier range for clinical data\")\n",
    "print(\"• 5-10%: Moderate concern - may indicate measurement issues or rare cases\")\n",
    "print(\"• >10%: Significant concern - requires investigation for data entry errors\")\n",
    "\n",
    "print(\"\\nCLINICAL SIGNIFICANCE ASSESSMENT:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Categorize features by outlier severity\n",
    "excellent_quality = outlier_df[outlier_df['IQR_Percentage'] < 2]\n",
    "good_quality = outlier_df[(outlier_df['IQR_Percentage'] >= 2) & (outlier_df['IQR_Percentage'] < 5)]\n",
    "moderate_concern = outlier_df[(outlier_df['IQR_Percentage'] >= 5) & (outlier_df['IQR_Percentage'] < 10)]\n",
    "significant_concern = outlier_df[outlier_df['IQR_Percentage'] >= 10]\n",
    "\n",
    "if len(excellent_quality) > 0:\n",
    "    print(f\"\\nEXCELLENT QUALITY ({len(excellent_quality)} features):\")\n",
    "    for _, row in excellent_quality.iterrows():\n",
    "        print(f\"   • {row['Feature']}: {row['IQR_Outliers']} outliers ({row['IQR_Percentage']}%)\")\n",
    "\n",
    "if len(good_quality) > 0:\n",
    "    print(f\"\\nGOOD QUALITY ({len(good_quality)} features):\")\n",
    "    for _, row in good_quality.iterrows():\n",
    "        print(f\"   • {row['Feature']}: {row['IQR_Outliers']} outliers ({row['IQR_Percentage']}%)\")\n",
    "\n",
    "if len(moderate_concern) > 0:\n",
    "    print(f\"\\n⚠️ MODERATE CONCERN ({len(moderate_concern)} features):\")\n",
    "    for _, row in moderate_concern.iterrows():\n",
    "        print(f\"   • {row['Feature']}: {row['IQR_Outliers']} outliers ({row['IQR_Percentage']}%)\")\n",
    "    print(\"   → Consider investigating these features for biological relevance vs. errors\")\n",
    "\n",
    "if len(significant_concern) > 0:\n",
    "    print(f\"\\nSIGNIFICANT CONCERN ({len(significant_concern)} features):\")\n",
    "    for _, row in significant_concern.iterrows():\n",
    "        print(f\"   • {row['Feature']}: {row['IQR_Outliers']} outliers ({row['IQR_Percentage']}%)\")\n",
    "    print(\"   → Recommend detailed investigation for data entry errors or systematic issues\")\n",
    "else:\n",
    "    print(\"\\nSIGNIFICANT CONCERN: None detected\")\n",
    "\n",
    "print(f\"\\nOVERALL DATA QUALITY ASSESSMENT:\")\n",
    "print(\"=\" * 35)\n",
    "total_features = len(outlier_df)\n",
    "good_features = len(excellent_quality) + len(good_quality)\n",
    "quality_score = (good_features / total_features) * 100\n",
    "\n",
    "print(f\"• Features with acceptable outlier levels: {good_features}/{total_features} ({quality_score:.1f}%)\")\n",
    "if quality_score >= 80:\n",
    "    print(\"• Overall Assessment: EXCELLENT data quality for clinical analysis\")\n",
    "elif quality_score >= 60:\n",
    "    print(\"• Overall Assessment: GOOD data quality with minor concerns\")\n",
    "else:\n",
    "    print(\"• Overall Assessment: REQUIRES ATTENTION before analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis\n",
    "\n",
    "### 4.1 Disease Stage Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze disease stage distribution\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Disease stage count plot\n",
    "plt.subplot(2, 2, 1)\n",
    "disease_counts = df_clean['Disease_Stage'].value_counts()\n",
    "plt.pie(disease_counts.values, labels=disease_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Distribution of Disease Stages')\n",
    "\n",
    "# Disease stage bar plot\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.countplot(data=df_clean, x='Disease_Stage', order=disease_counts.index)\n",
    "plt.title('Count of Patients by Disease Stage')\n",
    "plt.xlabel('Disease Stage')\n",
    "plt.ylabel('Number of Patients')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Age distribution by disease stage\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.boxplot(data=df_clean, x='Disease_Stage', y='Age')\n",
    "plt.title('Age Distribution by Disease Stage')\n",
    "plt.xlabel('Disease Stage')\n",
    "plt.ylabel('Age')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# HTT CAG repeat length by disease stage\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.boxplot(data=df_clean, x='Disease_Stage', y='HTT_CAG_Repeat_Length')\n",
    "plt.title('HTT CAG Repeat Length by Disease Stage')\n",
    "plt.xlabel('Disease Stage')\n",
    "plt.ylabel('HTT CAG Repeat Length')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# summary statistics\n",
    "print(\"Disease Stage Distribution:\")\n",
    "print(\"=\" * 40)\n",
    "for stage, count in disease_counts.items():\n",
    "    percentage = (count / len(df_clean)) * 100\n",
    "    print(f\"{stage}: {count:,} patients ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTotal patients analyzed: {len(df_clean):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Clinical Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze key clinical features\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# Set consistent color palette\n",
    "colors = ['skyblue', 'lightgreen', 'coral', 'plum', 'gold', 'lightsteelblue', 'lightblue', 'pink']\n",
    "\n",
    "# 1. Age distribution\n",
    "plt.subplot(3, 3, 1)\n",
    "plt.hist(df_clean['Age'], bins=30, alpha=0.7, color=colors[0], edgecolor='black', linewidth=0.5)\n",
    "plt.title('Age Distribution', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Age (years)', fontsize=10)\n",
    "plt.ylabel('Frequency', fontsize=10)\n",
    "\n",
    "# 2. HTT CAG Repeat Length distribution\n",
    "plt.subplot(3, 3, 2)\n",
    "plt.hist(df_clean['HTT_CAG_Repeat_Length'], bins=30, alpha=0.7, color=colors[1], edgecolor='black', linewidth=0.5)\n",
    "plt.title('HTT CAG Repeat Length Distribution', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('CAG Repeats', fontsize=10)\n",
    "plt.ylabel('Frequency', fontsize=10)\n",
    "\n",
    "# 3. Chorea Score distribution\n",
    "plt.subplot(3, 3, 3)\n",
    "plt.hist(df_clean['Chorea_Score'], bins=30, alpha=0.7, color=colors[2], edgecolor='black', linewidth=0.5)\n",
    "plt.title('Chorea Score Distribution', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Chorea Score', fontsize=10)\n",
    "plt.ylabel('Frequency', fontsize=10)\n",
    "\n",
    "# 4. Brain Volume Loss distribution\n",
    "plt.subplot(3, 3, 4)\n",
    "plt.hist(df_clean['Brain_Volume_Loss'], bins=30, alpha=0.7, color=colors[3], edgecolor='black', linewidth=0.5)\n",
    "plt.title('Brain Volume Loss Distribution', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Brain Volume Loss (%)', fontsize=10)\n",
    "plt.ylabel('Frequency', fontsize=10)\n",
    "\n",
    "# 5. Functional Capacity distribution\n",
    "plt.subplot(3, 3, 5)\n",
    "plt.hist(df_clean['Functional_Capacity'], bins=30, alpha=0.7, color=colors[4], edgecolor='black', linewidth=0.5)\n",
    "plt.title('Functional Capacity Distribution', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Functional Capacity', fontsize=10)\n",
    "plt.ylabel('Frequency', fontsize=10)\n",
    "\n",
    "# 6. Motor Symptoms by Disease Stage\n",
    "plt.subplot(3, 3, 6)\n",
    "motor_crosstab = pd.crosstab(df_clean['Disease_Stage'], df_clean['Motor_Symptoms'])\n",
    "motor_crosstab.plot(kind='bar', stacked=True, ax=plt.gca(), color=['lightcoral', 'lightgreen'])\n",
    "plt.title('Motor Symptoms by Disease Stage', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Disease Stage', fontsize=10)\n",
    "plt.ylabel('Count', fontsize=10)\n",
    "plt.xticks(rotation=45, fontsize=9)\n",
    "plt.legend(title='Motor Symptoms', bbox_to_anchor=(1.05, 1), loc='upper left', fontsize=9)\n",
    "\n",
    "# 7. Sex distribution\n",
    "plt.subplot(3, 3, 7)\n",
    "sex_counts = df_clean['Sex'].value_counts()\n",
    "plt.pie(sex_counts.values, labels=sex_counts.index, autopct='%1.1f%%', colors=['lightblue', 'pink'],\n",
    "        startangle=90, textprops={'fontsize': 10})\n",
    "plt.title('Sex Distribution', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 8. Family History distribution\n",
    "plt.subplot(3, 3, 8)\n",
    "family_counts = df_clean['Family_History'].value_counts()\n",
    "plt.pie(family_counts.values, labels=family_counts.index, autopct='%1.1f%%', colors=['lightcoral', 'lightyellow'],\n",
    "        startangle=90, textprops={'fontsize': 10})\n",
    "plt.title('Family History Distribution', fontsize=12, fontweight='bold')\n",
    "\n",
    "# 9. Gene Mutation Type distribution\n",
    "plt.subplot(3, 3, 9)\n",
    "mutation_counts = df_clean['Gene_Mutation_Type'].value_counts()\n",
    "plt.bar(range(len(mutation_counts)), mutation_counts.values, color=colors[5])\n",
    "plt.title('Gene Mutation Type Distribution', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Mutation Type', fontsize=10)\n",
    "plt.ylabel('Count', fontsize=10)\n",
    "plt.xticks(range(len(mutation_counts)), mutation_counts.index, rotation=45, fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Correlation Analysis and Feature Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis with statistical significance testing\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "# Calculate correlation matrix with p-values\n",
    "numerical_features = df_clean.select_dtypes(include=[np.number]).columns\n",
    "correlation_matrix = df_clean[numerical_features].corr()\n",
    "\n",
    "# Calculate p-values for correlations\n",
    "p_values = np.zeros((len(numerical_features), len(numerical_features)))\n",
    "for i, col1 in enumerate(numerical_features):\n",
    "    for j, col2 in enumerate(numerical_features):\n",
    "        if i != j:\n",
    "            clean_data1 = df_clean[col1].dropna()\n",
    "            clean_data2 = df_clean[col2].dropna()\n",
    "            common_idx = clean_data1.index.intersection(clean_data2.index)\n",
    "            if len(common_idx) > 3:\n",
    "                _, p_val = pearsonr(df_clean.loc[common_idx, col1], df_clean.loc[common_idx, col2])\n",
    "                p_values[i, j] = p_val\n",
    "            else:\n",
    "                p_values[i, j] = 1.0\n",
    "        else:\n",
    "            p_values[i, j] = 0.0\n",
    "\n",
    "# Create significance mask (p < 0.05)\n",
    "significant_mask = p_values < 0.05\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.subplot(2, 3, 1)\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='RdYlBu_r', center=0, \n",
    "            square=True, fmt='.2f', cbar_kws={'label': 'Correlation Coefficient'},\n",
    "            mask=~significant_mask, annot_kws={'size': 8})\n",
    "plt.title('Correlation Matrix (p < 0.05)', fontsize=12, pad=10)\n",
    "plt.xticks(rotation=45, ha='right', fontsize=9)\n",
    "plt.yticks(rotation=0, fontsize=9)\n",
    "\n",
    "#HTT CAG Repeats vs Age - genetic load relationship\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.scatter(df_clean['Age'], df_clean['HTT_CAG_Repeat_Length'], alpha=0.6, color='darkred')\n",
    "plt.xlabel('Age (years)', fontsize=10)\n",
    "plt.ylabel('HTT CAG Repeat Length', fontsize=10)\n",
    "plt.title('Genetic Load vs Age', fontsize=12)\n",
    "# trend line\n",
    "z = np.polyfit(df_clean['Age'], df_clean['HTT_CAG_Repeat_Length'], 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(df_clean['Age'], p(df_clean['Age']), \"r--\", alpha=0.8, linewidth=2)\n",
    "\n",
    "# Chorea Score vs Brain Volume Loss - disease severity relationship\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.scatter(df_clean['Chorea_Score'], df_clean['Brain_Volume_Loss'], alpha=0.6, color='navy')\n",
    "plt.xlabel('Chorea Score', fontsize=10)\n",
    "plt.ylabel('Brain Volume Loss', fontsize=10)\n",
    "plt.title('Movement Disorders vs Brain Atrophy', fontsize=12)\n",
    "# Add trend line\n",
    "z = np.polyfit(df_clean['Chorea_Score'], df_clean['Brain_Volume_Loss'], 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(df_clean['Chorea_Score'], p(df_clean['Chorea_Score']), \"r--\", alpha=0.8, linewidth=2)\n",
    "\n",
    "# functional Capacity vs HTT CAG Repeats - genetic impact on function\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.scatter(df_clean['HTT_CAG_Repeat_Length'], df_clean['Functional_Capacity'], alpha=0.6, color='green')\n",
    "plt.xlabel('HTT CAG Repeat Length', fontsize=10)\n",
    "plt.ylabel('Functional Capacity', fontsize=10)\n",
    "plt.title('Genetic Burden vs Daily Function', fontsize=12)\n",
    "# Add trend line\n",
    "z = np.polyfit(df_clean['HTT_CAG_Repeat_Length'], df_clean['Functional_Capacity'], 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(df_clean['HTT_CAG_Repeat_Length'], p(df_clean['HTT_CAG_Repeat_Length']), \"r--\", alpha=0.8, linewidth=2)\n",
    "\n",
    "# Age vs Brain Volume Loss - aging and neurodegeneration\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.scatter(df_clean['Age'], df_clean['Brain_Volume_Loss'], alpha=0.6, color='purple')\n",
    "plt.xlabel('Age (years)', fontsize=10)\n",
    "plt.ylabel('Brain Volume Loss', fontsize=10)\n",
    "plt.title('Age-Related Brain Atrophy', fontsize=12)\n",
    "# trend line\n",
    "z = np.polyfit(df_clean['Age'], df_clean['Brain_Volume_Loss'], 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(df_clean['Age'], p(df_clean['Age']), \"r--\", alpha=0.8, linewidth=2)\n",
    "\n",
    "# Protein Aggregation vs HTT Expression - molecular pathology\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.scatter(df_clean['HTT_Gene_Expression_Level'], df_clean['Protein_Aggregation_Level'], alpha=0.6, color='orange')\n",
    "plt.xlabel('HTT Gene Expression Level', fontsize=10)\n",
    "plt.ylabel('Protein Aggregation Level', fontsize=10)\n",
    "plt.title('Gene Expression vs Protein Pathology', fontsize=12)\n",
    "#trend line\n",
    "z = np.polyfit(df_clean['HTT_Gene_Expression_Level'], df_clean['Protein_Aggregation_Level'], 1)\n",
    "p = np.poly1d(z)\n",
    "plt.plot(df_clean['HTT_Gene_Expression_Level'], p(df_clean['HTT_Gene_Expression_Level']), \"r--\", alpha=0.8, linewidth=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Research Questions and Insights\n",
    "\n",
    "### 5.1 Key Findings from EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summary of key statistical findings\n",
    "print(\"HUNTINGTON'S DISEASE DATA ANALYSIS - KEY INSIGHTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# data quality insights\n",
    "print(\"DATA QUALITY INSIGHTS:\")\n",
    "print(\"=\" * 30)\n",
    "total_patients = len(df_clean)\n",
    "missing_cognitive = df_clean['Cognitive_Decline'].isnull().sum()\n",
    "missing_pct = (missing_cognitive / total_patients) * 100\n",
    "\n",
    "print(f\"• Total patients analyzed: {total_patients:,}\")\n",
    "print(f\"• Missing cognitive assessments: {missing_cognitive:,} ({missing_pct:.1f}%)\")\n",
    "print(f\"• Complete clinical profiles: {total_patients - missing_cognitive:,}\")\n",
    "\n",
    "# disease stage distribution insights\n",
    "print(f\"\\nDISEASE STAGE DISTRIBUTION:\")\n",
    "print(\"=\" * 30)\n",
    "stage_dist = df_clean['Disease_Stage'].value_counts()\n",
    "for stage, count in stage_dist.items():\n",
    "    pct = (count / total_patients) * 100\n",
    "    print(f\"• {stage}: {count:,} patients ({pct:.1f}%)\")\n",
    "\n",
    "# demographic insights\n",
    "print(f\"\\nDEMOGRAPHIC PATTERNS:\")\n",
    "print(\"=\" * 30)\n",
    "sex_dist = df_clean['Sex'].value_counts()\n",
    "family_hist = df_clean['Family_History'].value_counts()\n",
    "\n",
    "for sex, count in sex_dist.items():\n",
    "    pct = (count / total_patients) * 100\n",
    "    print(f\"• {sex}: {count:,} patients ({pct:.1f}%)\")\n",
    "\n",
    "print(f\"• Family History Present: {family_hist.get('Yes', 0):,} patients ({(family_hist.get('Yes', 0)/total_patients)*100:.1f}%)\")\n",
    "\n",
    "# clinical severity patterns\n",
    "print(f\"\\nCLINICAL SEVERITY PATTERNS:\")\n",
    "print(\"=\" * 30)\n",
    "print(f\"• Age range: {df_clean['Age'].min()}-{df_clean['Age'].max()} years (mean: {df_clean['Age'].mean():.1f})\")\n",
    "print(f\"• HTT CAG repeats: {df_clean['HTT_CAG_Repeat_Length'].min()}-{df_clean['HTT_CAG_Repeat_Length'].max()} (mean: {df_clean['HTT_CAG_Repeat_Length'].mean():.1f})\")\n",
    "print(f\"• Chorea severity: {df_clean['Chorea_Score'].min():.1f}-{df_clean['Chorea_Score'].max():.1f} (mean: {df_clean['Chorea_Score'].mean():.1f})\")\n",
    "print(f\"• Brain volume loss: {df_clean['Brain_Volume_Loss'].min():.1f}-{df_clean['Brain_Volume_Loss'].max():.1f}% (mean: {df_clean['Brain_Volume_Loss'].mean():.1f}%)\")\n",
    "\n",
    "# correlation insights\n",
    "print(f\"\\nCORRELATION INSIGHTS:\")\n",
    "print(\"=\" * 30)\n",
    "corr_matrix = df_clean[numerical_features].corr()\n",
    "\n",
    "#strongest correlations with key clinical variables\n",
    "key_correlations = [\n",
    "    ('HTT_CAG_Repeat_Length', 'Chorea_Score'),\n",
    "    ('Age', 'Brain_Volume_Loss'),\n",
    "    ('Chorea_Score', 'Brain_Volume_Loss'),\n",
    "    ('HTT_CAG_Repeat_Length', 'Functional_Capacity'),\n",
    "    ('HTT_Gene_Expression_Level', 'Protein_Aggregation_Level')\n",
    "]\n",
    "\n",
    "for var1, var2 in key_correlations:\n",
    "    correlation = corr_matrix.loc[var1, var2]\n",
    "    direction = \"positive\" if correlation > 0 else \"negative\"\n",
    "    strength = \"strong\" if abs(correlation) > 0.7 else \"moderate\" if abs(correlation) > 0.4 else \"weak\"\n",
    "    print(f\"• {var1} ↔ {var2}: {correlation:.3f} ({strength} {direction})\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Potential Research Questions Based on EDA Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Research Questions Based on EDA Findings\n",
    "print(\"RESEARCH QUESTIONS FOR HUNTINGTON'S DISEASE DATASET\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "print(\"Based on comprehensive EDA analysis, the following research questions\")\n",
    "print(\"could be explored using this dataset:\\n\")\n",
    "\n",
    "research_questions = [\n",
    "    {\n",
    "        \"category\": \"GENETIC PREDICTION & PROGNOSIS\",\n",
    "        \"questions\": [\n",
    "            \"Can HTT CAG repeat length predict disease onset timing in pre-symptomatic patients?\",\n",
    "            \"What is the optimal CAG repeat threshold for early intervention strategies?\",\n",
    "            \"How does genetic mutation type influence disease progression patterns?\",\n",
    "            \"Can we develop a genetic risk score combining CAG repeats and mutation type?\"\n",
    "        ],\n",
    "        \"eda_support\": \"HTT CAG repeats show correlations with clinical severity measures and disease stages.\"\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"DISEASE PROGRESSION MODELING\", \n",
    "        \"questions\": [\n",
    "            \"Can we predict functional decline trajectory using baseline clinical measures?\",\n",
    "            \"What combination of biomarkers best predicts transition between disease stages?\",\n",
    "            \"How does brain volume loss correlate with motor and cognitive symptom severity?\",\n",
    "            \"Can we identify rapid vs. slow progressors using early clinical indicators?\"\n",
    "        ],\n",
    "        \"eda_support\": \"Strong correlations found between age, brain atrophy, and clinical severity scores.\"\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"DEMOGRAPHIC & CLINICAL PATTERNS\",\n",
    "        \"questions\": [\n",
    "            \"Do sex differences influence disease presentation and progression patterns?\",\n",
    "            \"How does family history impact age of symptom onset and severity?\",\n",
    "            \"What are the optimal clinical assessment schedules for different patient subgroups?\",\n",
    "            \"Can demographic factors improve personalized treatment planning?\"\n",
    "        ],\n",
    "        \"eda_support\": \"Demographic distributions show balanced representation across sex and family history.\"\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"BIOMARKER DEVELOPMENT\",\n",
    "        \"questions\": [\n",
    "            \"Can protein aggregation levels serve as early biomarkers for disease progression?\",\n",
    "            \"What is the relationship between HTT gene expression and clinical outcomes?\",\n",
    "            \"Can we develop composite biomarker scores for clinical trials?\",\n",
    "            \"Which molecular markers correlate best with functional capacity changes?\"\n",
    "        ],\n",
    "        \"eda_support\": \"Molecular features (gene expression, protein aggregation) show measurable relationships.\"\n",
    "    },\n",
    "    {\n",
    "        \"category\": \"CLINICAL TRIAL DESIGN\",\n",
    "        \"questions\": [\n",
    "            \"What sample sizes are needed for detecting treatment effects in different disease stages?\",\n",
    "            \"Which clinical endpoints show the most sensitivity to change over time?\",\n",
    "            \"Can we stratify patients for clinical trials based on progression risk profiles?\",\n",
    "            \"What are optimal inclusion/exclusion criteria for therapeutic studies?\"\n",
    "        ],\n",
    "        \"eda_support\": \"Large dataset (48k+ patients) with balanced stage distribution enables power calculations.\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, rq in enumerate(research_questions, 1):\n",
    "    print(f\"{rq['category']}\")\n",
    "    print(\"-\" * len(rq['category']))\n",
    "    \n",
    "    for j, question in enumerate(rq['questions'], 1):\n",
    "        print(f\"   {j}. {question}\")\n",
    "    \n",
    "    print(f\"EDA Support: {rq['eda_support']}\\n\")\n",
    "\n",
    "print(\"RECOMMENDED PRIORITY RESEARCH AREAS:\")\n",
    "print(\"=\" * 45)\n",
    "print(\"1. GENETIC PREDICTION MODELS - High clinical impact for patient counseling\")\n",
    "print(\"2. BIOMARKER VALIDATION - Critical for drug development pipeline\") \n",
    "print(\"3. PROGRESSION MODELING - Essential for clinical trial design\")\n",
    "print(\"4. PERSONALIZED MEDICINE - Future of HD patient care\")\n",
    "\n",
    "print(f\"\\nDATASET STRENGTHS FOR RESEARCH:\")\n",
    "print(\"=\" * 35)\n",
    "print(\"- Large sample size (48,536 patients) provides statistical power\")\n",
    "print(\"- Comprehensive clinical features enable multi-modal analysis\")  \n",
    "print(\"- Balanced disease stage representation supports longitudinal insights\")\n",
    "print(\"- Genetic and molecular data enable precision medicine approaches\")\n",
    "print(\"- Missing data patterns are manageable (only 25% cognitive assessments)\")\n",
    "\n",
    "print(f\"\\nDATASET LIMITATIONS TO CONSIDER:\")\n",
    "print(\"=\" * 40)\n",
    "print(\"- Cross-sectional data limits longitudinal progression analysis\")\n",
    "print(\"- Missing cognitive assessments may bias severity estimates\")\n",
    "print(\"- Synthetic molecular sequences limit translational applications\")\n",
    "print(\"- Geographic/ethnic diversity not captured in current features\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"CONCLUSION: This dataset provides a solid foundation for multiple\")\n",
    "print(\"   research directions in Huntington's disease, with particular strength\")\n",
    "print(\"   in genetic prediction and clinical progression modeling.\")\n",
    "print(\"=\" * 65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Save Cleaned Data for Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save cleaned dataset for Assignment 3\n",
    "df_clean.to_csv('data/Huntington_Disease_Cleaned.csv', index=False)\n",
    "print(f\"Cleaned dataset saved: {df_clean.shape[0]:,} patients, {df_clean.shape[1]} features\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
