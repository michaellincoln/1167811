{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Huntington's Disease Dataset Analysis\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core data manipulation and analysis libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistical analysis\n",
    "from scipy import stats\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options for better DataFrame output\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Set plotting style for better visualizations\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "df = pd.read_csv('data/Huntington_Disease_Dataset.csv')\n",
    "\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"Number of patients: {df.shape[0]}\")\n",
    "print(f\"Number of features: {df.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display first few rows to see the data structure\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#detailed information about the dataset\n",
    "#to check data types, missing values, and memory usage\n",
    "print(\"Dataset Information:\")\n",
    "print(\"=\" * 50)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#statistical summary of numerical columns\n",
    "#to provide insights into the distribution of key clinical variables\n",
    "print(\"Statistical Summary of Numerical Features:\")\n",
    "print(\"=\" * 50)\n",
    "df.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#identify column types and their relevance for analysis\n",
    "\n",
    "print(\"Column Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "for i, col in enumerate(df.columns):\n",
    "    dtype = df[col].dtype\n",
    "    unique_vals = df[col].nunique()\n",
    "    missing_vals = df[col].isnull().sum()\n",
    "    print(f\"{i+1:2d}. {col:<30} | {str(dtype):<10} | Unique: {unique_vals:4d} | Missing: {missing_vals:3d}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preprocessing\n",
    "\n",
    "### 3.1 Data Cleaning and Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test to remove irrelevant columns for analysis\n",
    "#patient_ID: unique identifier, not predictive\n",
    "#random sequences: generated for privacy, not real biological data\n",
    "#gene info columns: redundant descriptive information\n",
    "#\n",
    "\n",
    "columns_to_drop = [\n",
    "    'Patient_ID',  # Unique identifier - not predictive\n",
    "    'Random_Protein_Sequence',  # Random sequence for privacy\n",
    "    'Random_Gene_Sequence',  # Random sequence for privacy  \n",
    "    'Gene/Factor',  # Redundant with other genetic features\n",
    "    'Chromosome_Location',  # Static genetic information\n",
    "    'Function',  # Descriptive, not quantitative\n",
    "    'Effect',  # Descriptive, not quantitative\n",
    "    'Category'  # Descriptive, not quantitative\n",
    "]\n",
    "\n",
    "# Create cleaned dataset focusing on clinically relevant features\n",
    "df_clean = df.drop(columns=columns_to_drop)\n",
    "\n",
    "print(f\"Original dataset: {df.shape}\")\n",
    "print(f\"Cleaned dataset: {df_clean.shape}\")\n",
    "print(f\"Removed {len(columns_to_drop)} irrelevant columns\")\n",
    "\n",
    "print(\"\\nRemaining features:\")\n",
    "for col in df_clean.columns:\n",
    "    print(f\"- {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Handle Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate records\n",
    "# In medical data, duplicates could indicate data entry errors\n",
    "\n",
    "print(\"Duplicate Analysis:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Check for complete duplicates\n",
    "duplicate_rows = df_clean.duplicated().sum()\n",
    "print(f\"Complete duplicate rows: {duplicate_rows}\")\n",
    "\n",
    "# Check for duplicates based on key clinical features\n",
    "key_features = ['Age', 'Sex', 'HTT_CAG_Repeat_Length', 'Disease_Stage']\n",
    "duplicate_clinical = df_clean.duplicated(subset=key_features).sum()\n",
    "print(f\"Duplicate clinical profiles: {duplicate_clinical}\")\n",
    "\n",
    "if duplicate_rows > 0:\n",
    "    print(f\"\\nRemoving {duplicate_rows} duplicate rows...\")\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    print(f\"Dataset shape after removing duplicates: {df_clean.shape}\")\n",
    "else:\n",
    "    print(\"No duplicate rows found - data quality is good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Missing Data Analysis and Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize missing data patterns\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Missing data heatmap\n",
    "plt.subplot(2, 2, 1)\n",
    "sns.heatmap(df_clean.isnull(), cbar=True, xticklabels=True, yticklabels=False, \n",
    "            cmap='viridis', cbar_kws={'label': 'Missing Data'})\n",
    "plt.title('Missing Data Heatmap')\n",
    "plt.xlabel('Features')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Missing data bar plot\n",
    "plt.subplot(2, 2, 2)\n",
    "missing_counts = df_clean.isnull().sum().sort_values(ascending=True)\n",
    "# Only show columns with missing data\n",
    "missing_counts = missing_counts[missing_counts > 0]  \n",
    "if len(missing_counts) > 0:\n",
    "    missing_counts.plot(kind='barh', color='coral')\n",
    "    plt.title('Missing Data Count by Feature')\n",
    "    plt.xlabel('Number of Missing Values')\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'No Missing Data Found!', ha='center', va='center', fontsize=14)\n",
    "    plt.title('Missing Data Count by Feature')\n",
    "\n",
    "# Missing data percentage\n",
    "plt.subplot(2, 2, 3)\n",
    "missing_percentages = ((df_clean.isnull().sum() / len(df_clean)) * 100).sort_values(ascending=True)\n",
    "missing_percentages = missing_percentages[missing_percentages > 0]\n",
    "if len(missing_percentages) > 0:\n",
    "    missing_percentages.plot(kind='barh', color='lightblue')\n",
    "    plt.title('Missing Data Percentage by Feature')\n",
    "    plt.xlabel('Percentage of Missing Values (%)')\n",
    "else:\n",
    "    plt.text(0.5, 0.5, 'No Missing Data Found!', ha='center', va='center', fontsize=14)\n",
    "    plt.title('Missing Data Percentage by Feature')\n",
    "\n",
    "# Data completeness overview\n",
    "plt.subplot(2, 2, 4)\n",
    "total_cells = len(df_clean) * len(df_clean.columns)\n",
    "missing_cells = df_clean.isnull().sum().sum()\n",
    "complete_cells = total_cells - missing_cells\n",
    "\n",
    "labels = ['Complete', 'Missing']\n",
    "sizes = [complete_cells, missing_cells]\n",
    "colors = ['lightgreen', 'lightcoral']\n",
    "\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Overall Data Completeness')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nData Completeness Summary:\")\n",
    "print(f\"Total data points: {total_cells:,}\")\n",
    "print(f\"Complete data points: {complete_cells:,} ({(complete_cells/total_cells)*100:.1f}%)\")\n",
    "print(f\"Missing data points: {missing_cells:,} ({(missing_cells/total_cells)*100:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive missing data analysis\n",
    "print(\"Missing Data Analysis:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Calculate missing data statistics\n",
    "missing_stats = []\n",
    "for col in df_clean.columns:\n",
    "    missing_count = df_clean[col].isnull().sum()\n",
    "    missing_percent = (missing_count / len(df_clean)) * 100\n",
    "    missing_stats.append({\n",
    "        'Column': col,\n",
    "        'Missing_Count': missing_count,\n",
    "        'Missing_Percent': round(missing_percent, 2),\n",
    "        'Data_Type': str(df_clean[col].dtype)\n",
    "    })\n",
    "\n",
    "# Create DataFrame for better visualization\n",
    "missing_df = pd.DataFrame(missing_stats)\n",
    "missing_df = missing_df.sort_values('Missing_Percent', ascending=False)\n",
    "\n",
    "print(\"Missing Data Summary:\")\n",
    "print(missing_df.to_string(index=False))\n",
    "\n",
    "# Identify columns with significant missing data (>5% missing)\n",
    "high_missing = missing_df[missing_df['Missing_Percent'] > 5]\n",
    "print(f\"\\nColumns with >5% missing data:\")\n",
    "if len(high_missing) > 0:\n",
    "    for _, row in high_missing.iterrows():\n",
    "        print(f\"- {row['Column']}: {row['Missing_Count']} missing ({row['Missing_Percent']}%)\")\n",
    "else:\n",
    "    print(\"- None (excellent data quality!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Outlier Detection and Handling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection\n",
    "# In medical data, outliers could represent: rare but valid extreme cases, data entry errors, measurement equipment issues\n",
    "\n",
    "def detect_outliers_iqr(data, column):\n",
    "    \"\"\"Detect outliers using Interquartile Range (IQR) method\"\"\"\n",
    "    Q1 = data[column].quantile(0.25)\n",
    "    Q3 = data[column].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = data[(data[column] < lower_bound) | (data[column] > upper_bound)]\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "def detect_outliers_zscore(data, column, threshold=3):\n",
    "    \"\"\"Detect outliers using Z-score method\"\"\"\n",
    "    z_scores = np.abs(stats.zscore(data[column].dropna()))\n",
    "    outliers = data[np.abs(stats.zscore(data[column].dropna())) > threshold]\n",
    "    return outliers, z_scores\n",
    "\n",
    "# Identify numerical columns for outlier analysis\n",
    "numerical_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(\"Numerical columns for outlier detection:\")\n",
    "for i, col in enumerate(numerical_cols, 1):\n",
    "    print(f\"{i:2d}. {col}\")\n",
    "\n",
    "print(f\"\\nAnalyzing {len(numerical_cols)} numerical features for outliers...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive outlier analysis\n",
    "outlier_summary = []\n",
    "\n",
    "print(\"Outlier Detection Summary:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Feature':<25} {'Total':<8} {'IQR_Out':<8} {'Z_Out':<8} {'%_IQR':<8} {'%_Z':<8}\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for col in numerical_cols:\n",
    "    #only analyze columns with data\n",
    "    if df_clean[col].notna().sum() > 0:  \n",
    "        # IQR method\n",
    "        iqr_outliers, lower_iqr, upper_iqr = detect_outliers_iqr(df_clean, col)\n",
    "        \n",
    "        # Z-score method  \n",
    "        zscore_outliers, z_scores = detect_outliers_zscore(df_clean, col)\n",
    "        \n",
    "        # Calculate percentages\n",
    "        total_valid = df_clean[col].notna().sum()\n",
    "        iqr_pct = (len(iqr_outliers) / total_valid) * 100\n",
    "        z_pct = (len(zscore_outliers) / total_valid) * 100\n",
    "        \n",
    "        # Store summary\n",
    "        outlier_summary.append({\n",
    "            'Feature': col,\n",
    "            'Total_Records': total_valid,\n",
    "            'IQR_Outliers': len(iqr_outliers),\n",
    "            'Z_Outliers': len(zscore_outliers),\n",
    "            'IQR_Percentage': round(iqr_pct, 2),\n",
    "            'Z_Percentage': round(z_pct, 2),\n",
    "            'Lower_Bound_IQR': round(lower_iqr, 2),\n",
    "            'Upper_Bound_IQR': round(upper_iqr, 2)\n",
    "        })\n",
    "        \n",
    "        print(f\"{col:<25} {total_valid:<8} {len(iqr_outliers):<8} {len(zscore_outliers):<8} {iqr_pct:<8.1f} {z_pct:<8.1f}\")\n",
    "\n",
    "# Convert to DataFrame for better analysis\n",
    "outlier_df = pd.DataFrame(outlier_summary)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Features with significant outliers (>5% of data):\")\n",
    "significant_outliers = outlier_df[outlier_df['IQR_Percentage'] > 5]\n",
    "if len(significant_outliers) > 0:\n",
    "    for _, row in significant_outliers.iterrows():\n",
    "        print(f\"- {row['Feature']}: {row['IQR_Outliers']} outliers ({row['IQR_Percentage']}%)\")\n",
    "else:\n",
    "    print(\"- No features have >5% outliers (good data quality!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis\n",
    "\n",
    "### 4.1 Disease Stage Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze disease stage distribution\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Disease stage count plot\n",
    "plt.subplot(2, 2, 1)\n",
    "disease_counts = df_clean['Disease_Stage'].value_counts()\n",
    "plt.pie(disease_counts.values, labels=disease_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "plt.title('Distribution of Disease Stages')\n",
    "\n",
    "# Disease stage bar plot\n",
    "plt.subplot(2, 2, 2)\n",
    "sns.countplot(data=df_clean, x='Disease_Stage', order=disease_counts.index)\n",
    "plt.title('Count of Patients by Disease Stage')\n",
    "plt.xlabel('Disease Stage')\n",
    "plt.ylabel('Number of Patients')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Age distribution by disease stage\n",
    "plt.subplot(2, 2, 3)\n",
    "sns.boxplot(data=df_clean, x='Disease_Stage', y='Age')\n",
    "plt.title('Age Distribution by Disease Stage')\n",
    "plt.xlabel('Disease Stage')\n",
    "plt.ylabel('Age')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# HTT CAG repeat length by disease stage\n",
    "plt.subplot(2, 2, 4)\n",
    "sns.boxplot(data=df_clean, x='Disease_Stage', y='HTT_CAG_Repeat_Length')\n",
    "plt.title('HTT CAG Repeat Length by Disease Stage')\n",
    "plt.xlabel('Disease Stage')\n",
    "plt.ylabel('HTT CAG Repeat Length')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# summary statistics\n",
    "print(\"Disease Stage Distribution:\")\n",
    "print(\"=\" * 40)\n",
    "for stage, count in disease_counts.items():\n",
    "    percentage = (count / len(df_clean)) * 100\n",
    "    print(f\"{stage}: {count:,} patients ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\nTotal patients analyzed: {len(df_clean):,}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
